{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Foundational code for TPOT\n",
    "\n",
    "Exploring the foundations of the Genetic Programming (GP) library TPOT, which automates the process of selecting the best machine learning model and hyperparameters for a given dataset. This notebook demonstrates the following foundational concepts:\n",
    "\n",
    "* Loading data from Elasticsearch\n",
    "* Preparing nested data for the data pipeline\n",
    "* Filtering out irrelevant information from traces\n",
    "* Vectorizing text data using BERT\n",
    "* Training a TPOT model\n",
    "* Evaluating the model and exporting the pipeline\n",
    "* Visualizing the frequency of models tested by TPOT\n",
    "* Loading the trained model and making predictions (todo)"
   ],
   "id": "9090fc8231b5aa47"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Function to recursively normalize nested columns in a DataFrame\n",
    "def recursively_normalize(data):\n",
    "    df = pd.json_normalize(data)\n",
    "    while True:\n",
    "        nested_cols = [col for col in df.columns if isinstance(df[col].iloc[0], (dict, list))]\n",
    "        if not nested_cols:\n",
    "            break\n",
    "        for col in nested_cols:\n",
    "            if isinstance(df[col].iloc[0], dict):\n",
    "                normalized = pd.json_normalize(df[col])\n",
    "                df = df.drop(columns=[col]).join(normalized)\n",
    "            elif isinstance(df[col].iloc[0], list):\n",
    "                df = df.explode(col)\n",
    "                normalized = pd.json_normalize(df[col])\n",
    "                df = df.drop(columns=[col]).join(normalized)\n",
    "    return df\n",
    "\n",
    "# Function to fetch the next batch using the cursor\n",
    "def fetch_next_batch(cursor):\n",
    "    response = requests.post(\n",
    "        f\"{base_url}/_sql?format=json\",\n",
    "        headers={\"Content-Type\": \"application/json\"},\n",
    "        json={\"cursor\": cursor}\n",
    "    ).json()\n",
    "    return response\n",
    "\n",
    "# Elasticsearch base URL\n",
    "base_url = \"http://192.168.20.106:9200\"\n",
    "# Index name\n",
    "index = \"winlogbeat-*\"\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Calculate the current time and the time one hour ago\n",
    "current_time = datetime.utcnow()\n",
    "one_hour_ago = current_time - timedelta(hours=1)\n",
    "\n",
    "# Format times in ISO8601 format as expected by Elasticsearch\n",
    "current_time_iso = current_time.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "one_hour_ago_iso = one_hour_ago.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "\n",
    "# SQL query with time filter\n",
    "sql_query = f\"\"\"\n",
    "SELECT \"@timestamp\", host.hostname, host.ip, log.level, winlog.event_id, winlog.task, message\n",
    "FROM \"winlogbeat-7.10.0-2024.06.23-*\"\n",
    "WHERE host.hostname = 'win10'\n",
    "AND winlog.provider_name = 'Microsoft-Windows-Sysmon'\n",
    "AND \"@timestamp\" >= '{one_hour_ago_iso}'\n",
    "AND \"@timestamp\" <= '{current_time_iso}'\n",
    "\"\"\"\n",
    "\n",
    "# Initial search request to start scrolling\n",
    "initial_response = requests.post(\n",
    "    f\"{base_url}/_sql?format=json\",\n",
    "    headers={\"Content-Type\": \"application/json\"},\n",
    "    json={\n",
    "        \"query\": sql_query,\n",
    "        \"field_multi_value_leniency\": True\n",
    "    }\n",
    ").json()\n",
    "\n",
    "# Extract the cursor for scrolling\n",
    "cursor = initial_response.get('cursor')\n",
    "rows = initial_response.get('rows')\n",
    "columns = [col['name'] for col in initial_response['columns']]\n",
    "\n",
    "# Initialize CSV file (assumes the first batch is not empty)\n",
    "if rows:\n",
    "    df = pd.DataFrame(rows, columns=columns)\n",
    "    df = recursively_normalize(df.to_dict(orient='records'))\n",
    "    df.to_csv(\"lab_logs_blindtest_activity.csv\", mode='w', index=False, header=True)\n",
    "\n",
    "# Track total documents retrieved\n",
    "total_documents_retrieved = len(rows)\n",
    "print(f\"Retrieved {total_documents_retrieved} documents.\")\n",
    "\n",
    "# Loop to fetch subsequent batches of documents until no more documents are left\n",
    "while cursor:\n",
    "    # Fetch next batch of documents using cursor\n",
    "    response = fetch_next_batch(cursor)\n",
    "    \n",
    "    # Update cursor for the next batch\n",
    "    cursor = response.get('cursor')\n",
    "    rows = response.get('rows')\n",
    "    \n",
    "    # If no rows, break out of the loop\n",
    "    if not rows:\n",
    "        break\n",
    "    \n",
    "    # Normalize data and append to CSV\n",
    "    df = pd.DataFrame(rows, columns=columns)\n",
    "    df = recursively_normalize(df.to_dict(orient='records'))\n",
    "    \n",
    "    # Append to CSV file without headers\n",
    "    df.to_csv(\"lab_logs_blindtest_activity.csv\", mode='a', index=False, header=False)\n",
    "    \n",
    "    # Convert DataFrame to JSON, line by line\n",
    "    json_lines = df.to_json(orient='records', lines=True).splitlines()\n",
    "    # Append each line to an existing JSON file\n",
    "    with open(\"lab_logs_blindtest_activity.json\", 'a') as file:\n",
    "        for line in json_lines:\n",
    "            file.write(line + '\\n')  # Append each line and add a newline\n",
    "        \n",
    "    # Update total documents retrieved\n",
    "    total_documents_retrieved += len(rows)\n",
    "    \n",
    "    print(f\"Retrieved {total_documents_retrieved} documents.\")\n",
    "\n",
    "print(\"Files have been written.\")"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Load data from a CSV file\n",
    "\n",
    "Load the data from the CSV file into a DataFrame using Polars, a fast DataFrame library in Rust. This step is necessary to prepare the data for further processing and filtering.\n"
   ],
   "id": "7dc4287c4b67a923"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import polars as pl\n",
    "\n",
    "# Define the path to your CSV file\n",
    "csv_file_path = 'lab_logs_blindtest_activity.csv'\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pl.read_csv(csv_file_path)\n",
    "\n",
    "# Show the DataFrame to confirm it's loaded correctly\n",
    "print(df)\n"
   ],
   "id": "847862813f6a8c74",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Data filtering and transformation\n",
    "\n",
    "Filter out irrelevant information from the traces to focus on the key details. This step involves removing specific lines based on keywords present at the start of the line. The goal is to clean up the data and make it more manageable for further processing."
   ],
   "id": "6fb9c9c06da8a061"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def remove_keyword_lines(batch, keywords):\n",
    "    def modify_line(line):\n",
    "        # Check each keyword; filter the line if the keyword is at the start followed by a colon\n",
    "        for keyword in keywords:\n",
    "            if line.startswith(f\"{keyword}:\"):\n",
    "                # Special handling for 'User' keyword\n",
    "                if keyword == 'User':\n",
    "                    parts = line.split('\\\\')\n",
    "                    if len(parts) > 1:\n",
    "                        return f\"User: {parts[1]}\"  # Only keep the part after the backslash\n",
    "                elif keyword == 'SourceHostname':\n",
    "                    parts = line.split('.')\n",
    "                    if len(parts) > 0:\n",
    "                        return f\"{keyword}: {parts[0].split(': ')[1]}\"  # Only keep the part before the first dot, remove keyword duplication\n",
    "                return None  # For other keywords, remove the line altogether\n",
    "        return line  # Return the line unchanged if no keyword conditions are met\n",
    "\n",
    "    # Use map_elements to apply a function to each message in the batch\n",
    "    return batch.map_elements(lambda message: '\\n'.join(\n",
    "        filter(None, (modify_line(line) for line in message.split('\\n')))), \n",
    "        return_dtype=pl.Utf8)\n",
    "\n",
    "\n",
    "#  keywords to filter or process\n",
    "keywords_to_filter = [\"UtcTime\", \"SourceProcessGUID\",\"ProcessGuid\", \"TargetProcessGUID\", \"TargetObject\", \"FileVersion\", \"Hashes\", \"LogonGuid\", \"LogonId\", \"CreationUtcTime\", \"User\", \"ParentProcessGuid\", \"SourceHostname\"]\n",
    "\n",
    "\n",
    "# Load the DataFrame (assuming 'df' is already loaded)\n",
    "# Apply the transformation to the 'message' column using map_batches\n",
    "df_f = df.with_columns(\n",
    "    pl.col(\"message\").map_batches(lambda batch: remove_keyword_lines(batch, keywords_to_filter), return_dtype=pl.Utf8).alias(\"filtered_message\")\n",
    ")\n",
    "\n",
    "# Assuming df_f is your DataFrame with the 'filtered_message' column\n",
    "# Fetch the first three rows from the 'filtered_message' column\n",
    "first_messages = df_f[\"filtered_message\"].head(200)\n",
    "\n",
    "# Print each message completely\n",
    "for i, message in enumerate(first_messages):\n",
    "    print(f\"Message {i+1}:\")\n",
    "    print(message)\n",
    "    print(\"-\" * 50)  # Separator for readability\n"
   ],
   "id": "fc93fe038bcb00c5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Select specific columns and write to a CSV file\n",
    "\n",
    "This is a data reduction approach where only the necessary columns are selected for further processing. The selected columns are then written to a new CSV file for use in subsequent steps."
   ],
   "id": "fa298e1c9d0999bd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Assuming df_f is your modified DataFrame with all necessary columns including 'filtered_message'\n",
    "# Select specific columns from the DataFrame\n",
    "selected_columns_df = df_f.select([\"log.level\", \"winlog.event_id\", \"winlog.task\",\"filtered_message\"])\n",
    "\n",
    "# Write the selected columns to a CSV file\n",
    "selected_columns_df.write_csv('lab_logs_blindtest_activity_filtered.csv')\n"
   ],
   "id": "ff54936e81a933fd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "selected_columns_df.head(5)",
   "id": "da3c38ca8c474ba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Indexing and inserting a new column\n",
    "\n",
    "The following code indexes the events in the dataframe and inserts the index as the first column. This step is essential for tracking the order of events and ensuring that the data remains organized throughout the process."
   ],
   "id": "b5eb69ab1b69523f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create an index series directly\n",
    "index_series = pl.Series(\"index\", range(selected_columns_df.height))\n",
    "\n",
    "# Insert the index series as the first column using the recommended method\n",
    "selected_columns_df = selected_columns_df.insert_column(0, index_series)\n",
    "\n",
    "# Write the DataFrame to a CSV file, including the new index column\n",
    "selected_columns_df.write_csv('lab_logs_blindtest_activity_filtered.csv')\n"
   ],
   "id": "35cd4cc645761608",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## TPOT model training and evaluation\n",
    "\n",
    "The following code demonstrates how to train a TPOT model using the data prepared in the previous steps. The model is trained on the vectorized text data and evaluated to determine its performance. The best model is then exported for future use."
   ],
   "id": "2173f7e8f3ae63a9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Install necessary libraries",
   "id": "2fbe4ebc4d9038a2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "%conda install numpy scipy scikit-learn pandas joblib pytorch",
   "id": "b3f6a7f89fb1f92e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "%pip install deap update_checker tqdm stopit xgboost",
   "id": "47de32d351fad54f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "%pip install tpot",
   "id": "737d462c559936e2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Initialize TPOT for Genetic Programming on the CPU\n",
    "\n",
    "The following code initializes a TPOT classifier for genetic programming on the CPU. The classifier is trained on the vectorized text data and evaluated to determine its performance. The best model is then exported for future use."
   ],
   "id": "ddf2807e5c8a393b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "import polars as pl\n",
    "import re\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from tpot import TPOTClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ],
   "id": "ae96e41f08c7908b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Building the feature vector\n",
    "\n",
    "Here a feature vector is build to extract the relevant features from Sysmon traces. The feature vector is then used to train the TPOT classifier."
   ],
   "id": "33c422b756ff0d9b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Extract relevant information using regular expressions\n",
    "def extract_info(text):\n",
    "    image = re.search(r\"Image: (.*?\\.exe)\", text, re.IGNORECASE)\n",
    "    target_filename = re.search(r\"TargetFilename: (.*?\\.exe)\", text, re.IGNORECASE)\n",
    "    return {\n",
    "        \"image\": image.group(1) if image else \"\",\n",
    "        \"target_filename\": target_filename.group(1) if target_filename else \"\",\n",
    "        \"text\": text\n",
    "    }"
   ],
   "id": "5cecd995c579cd0f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Apply extraction to the Polars DataFrame using map_elements\n",
    "selected_columns_df = selected_columns_df.with_columns(\n",
    "    pl.col(\"filtered_message\").map_elements(lambda x: extract_info(x), return_dtype=pl.Object).alias(\"extracted_info\")\n",
    ")"
   ],
   "id": "c2f84d1d644f9111",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Extract fields from the extracted_info column using map_elements with return_dtype\n",
    "selected_columns_df = selected_columns_df.with_columns(\n",
    "    pl.col(\"extracted_info\").map_elements(lambda x: x['image'], return_dtype=pl.Utf8).alias(\"image\"),\n",
    "    pl.col(\"extracted_info\").map_elements(lambda x: x['target_filename'], return_dtype=pl.Utf8).alias(\"target_filename\"),\n",
    "    pl.col(\"extracted_info\").map_elements(lambda x: x['text'], return_dtype=pl.Utf8).alias(\"text\")\n",
    ").drop(\"extracted_info\")"
   ],
   "id": "b4c8e805cdb9b634",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(selected_columns_df)",
   "id": "c700056897cc8dd8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "####  Define the label based on conditions\n",
    "\n",
    "The following code defines the label based on specific conditions. The conditions are applied to the image and target_filename columns to determine whether the event is malicious or benign. The label is then assigned accordingly. This step is crucial for training the TPOT classifier.\n",
    "\n",
    "This is a single-label classification problem, where the label is binary (good or bad)."
   ],
   "id": "3df9414538271fdc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def define_label(row):\n",
    "    conditions = {\n",
    "        (\"EXCEL.EXE\" in row['image'] and \".exe\" in row['target_filename']): \"bad\",\n",
    "        (row['index'] == 874): \"bad\",\n",
    "        # Add more conditions here if needed\n",
    "    }\n",
    "    return conditions.get(True, \"good\")"
   ],
   "id": "8d21ff3214accd7a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Apply the define_label function\n",
    "selected_columns_df = selected_columns_df.with_columns(\n",
    "    pl.struct([\"index\", \"image\", \"target_filename\"]).map_elements(define_label, return_dtype=pl.Utf8).alias(\"label\")\n",
    ")"
   ],
   "id": "3017223325f75d03",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(selected_columns_df)",
   "id": "feac611ac2db9fb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "bad_rows = selected_columns_df.filter(pl.col(\"label\") == \"bad\")\n",
    "print(bad_rows)"
   ],
   "id": "5d634a8db0b99c4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Vectorizing the text data using BERT\n",
    "\n",
    "The following code demonstrates how to vectorize the text data using BERT. The vectorized text data is then used as input for the TPOT classifier. The BERT model is loaded and applied to the text column in the DataFrame to generate the feature vector."
   ],
   "id": "a4697a39b64b182f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def vectorize_text(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
    "    outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "\n",
    "# Apply vectorization to the Polars DataFrame using map_elements\n",
    "selected_columns_df = selected_columns_df.with_columns(\n",
    "    pl.col(\"text\").map_elements(lambda x: vectorize_text(x).flatten(), return_dtype=pl.Object).alias(\"text_vector\")\n",
    ")\n",
    "\n",
    "print(selected_columns_df)"
   ],
   "id": "9262f948e3361ee9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df = selected_columns_df.to_pandas()\n",
    "\n",
    "# Save the Pandas DataFrame to a Parquet file\n",
    "df.to_parquet(\"vectorized_texts.parquet\")"
   ],
   "id": "91e007e2b208dc7f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "# Load the DataFrame from the Parquet file\n",
    "loaded_df = pd.read_parquet(\"vectorized_texts.parquet\")\n",
    "\n",
    "# Verify the loaded DataFrame\n",
    "print(loaded_df)"
   ],
   "id": "48a10b20636b4a2d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tpot import TPOTClassifier\n",
    "\n",
    "# Load the DataFrame from the Parquet file\n",
    "df = pd.read_parquet(\"vectorized_texts.parquet\")\n",
    "\n",
    "# Ensure to use only CPU for PyTorch\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "df['label_encoded'] = le.fit_transform(df['label'])\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text_vector'].tolist(), df['label_encoded'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "X_train = torch.tensor(X_train, device=device).numpy()\n",
    "X_test = torch.tensor(X_test, device=device).numpy()\n",
    "\n",
    "# TPOT classifier with higher verbosity\n",
    "tpot = TPOTClassifier(verbosity=3, generations=5, population_size=20)\n",
    "tpot.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"TPOT Score:\", tpot.score(X_test, y_test))\n",
    "\n",
    "# Save the trained model\n",
    "tpot.export('tpot_pipeline.py')\n",
    "\n",
    "# Print the exported pipeline\n",
    "with open('tpot_pipeline.py') as f:\n",
    "    print(f.read())\n",
    "\n",
    "# Example of using the trained model\n",
    "predictions = tpot.predict(X_test)\n",
    "print(\"Predictions:\", predictions)\n"
   ],
   "id": "75d84e297b03eaf4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(\"The accuracy of the best model is: \", tpot.score(X_test, y_test))\n",
   "id": "6cf76b5736411710",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "%pip install matplotlib",
   "id": "d99c8aa5529a72d1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tpot import TPOTClassifier\n",
    "from collections import Counter\n",
    "\n",
    "# Load the DataFrame from the Parquet file\n",
    "df = pd.read_parquet(\"vectorized_texts.parquet\")\n",
    "\n",
    "# Ensure to use only CPU for PyTorch\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "df['label_encoded'] = le.fit_transform(df['label'])\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text_vector'].tolist(), df['label_encoded'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "X_train = torch.tensor(X_train, device=device).numpy()\n",
    "X_test = torch.tensor(X_test, device=device).numpy()\n",
    "\n",
    "# TPOT classifier with higher verbosity\n",
    "tpot = TPOTClassifier(verbosity=3, generations=5, population_size=20)\n",
    "tpot.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"TPOT Score:\", tpot.score(X_test, y_test))\n",
    "\n",
    "# Save the trained model\n",
    "tpot.export('tpot_pipeline.py')\n",
    "\n",
    "# Print the exported pipeline\n",
    "with open('tpot_pipeline.py') as f:\n",
    "    print(f.read())\n",
    "\n",
    "# Example of using the trained model\n",
    "predictions = tpot.predict(X_test)\n",
    "print(\"Predictions:\", predictions)\n",
    "\n",
    "# Extract information about models tested\n",
    "evaluated_pipelines = tpot.evaluated_individuals_\n"
   ],
   "id": "705690ce71dfda4c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Count occurrences of each model type\n",
    "model_counter = Counter()\n",
    "for pipeline_str in evaluated_pipelines.keys():\n",
    "    models = re.findall(r'\\w+\\(.*?\\)', pipeline_str)\n",
    "    for model in models:\n",
    "        model_name = model.split('(')[0]\n",
    "        model_counter[model_name] += 1\n",
    "\n",
    "print(\"Models and their occurrences:\")\n",
    "for model, count in model_counter.items():\n",
    "    print(f\"{model}: {count}\")\n",
    "\n",
    "# Visualize the count of different models\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model_names = list(model_counter.keys())\n",
    "model_counts = list(model_counter.values())\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(model_names, model_counts, color='skyblue')\n",
    "plt.xlabel('Number of Occurrences')\n",
    "plt.ylabel('Model')\n",
    "plt.title('Frequency of Models Tested by TPOT')\n",
    "plt.show()"
   ],
   "id": "565066bf3b5f0820",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "f6faa6d6265c094e",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
